{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PNML Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sample 2D dataset\n",
    "One half is going to be unexplored, similar to the Easy maze task. Then, we'll:\n",
    "1. collect goal examples in the unexplored region\n",
    "2. train the goal classifier (offline) similar to VICE\n",
    "3. query test points (training an extra logistic regression layer on top for points in and out of the region to check what the reward is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4x4 pointmass environment, sampling from the left half\n",
    "data = np.random.uniform([-4, -4], [0, 4], size=(500, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = np.array([2.5, -2.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.5)\n",
    "plt.scatter(goal[0], goal[1], marker='*', s=100)\n",
    "plt.title('Data Collected by Policy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = np.random.uniform(goal - 0.1, goal + 0.1, size=(100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.scatter(positives[:, 0], positives[:, 1], alpha=0.5)\n",
    "plt.scatter(goal[0], goal[1], marker='*', s=100)\n",
    "plt.title('Collected Positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train Goal Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softlearning.models.vice_models import create_feedforward_reward_classifier_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes = collections.OrderedDict({\n",
    "    'state_obesrvation': np.array([2]),\n",
    "})\n",
    "\n",
    "classifier = create_feedforward_reward_classifier_function(\n",
    "    input_shapes=input_shapes,\n",
    "    hidden_layer_sizes=(64, ),\n",
    "    activation='linear',\n",
    "    output_activation='sigmoid',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.layers[1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "def loss(model, x, y, training):\n",
    "    # training=training is needed only if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    y_ = model(x, training=training)\n",
    "\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, training=True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "\n",
    "num_epochs = 101\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "    for i in range(10):\n",
    "        negatives_batch_idx = np.random.choice(data.shape[0], size=32)\n",
    "        positives_batch_idx = np.random.choice(positives.shape[0], size=32)\n",
    "        negatives_batch = data[negatives_batch_idx]\n",
    "        positives_batch = positives[positives_batch_idx]\n",
    "        \n",
    "        x = np.vstack((negatives_batch, positives_batch))\n",
    "        y = np.vstack((\n",
    "            np.zeros((len(negatives_batch), 1)),\n",
    "            np.ones((len(positives_batch), 1)),\n",
    "        ))\n",
    "        \n",
    "        loss_value, grads = grad(classifier, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, classifier.trainable_variables))\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "\n",
    "    # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.6f}\".format(epoch, epoch_loss_avg.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "xs = np.linspace(-4, 4, n_samples)\n",
    "ys = np.linspace(-4, 4, n_samples)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "xys = np.meshgrid(xs, ys)\n",
    "grid_vals = np.array(xys).transpose(1, 2, 0).reshape((n_samples * n_samples, 2))\n",
    "\n",
    "rewards = classifier.predict(grid_vals)\n",
    "\n",
    "plt.contourf(xys[0], xys[1], rewards.reshape(xys[0].shape), levels=20)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.scatter(goal[0], goal[1], color='red', marker='*', s=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pNML Reward Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training_iter(model, optimizer, test_point=None, positive=True, batch_size=64):\n",
    "    negatives_batch_idx = np.random.choice(data.shape[0], size=batch_size)\n",
    "    positives_batch_idx = np.random.choice(positives.shape[0], size=batch_size)\n",
    "    negatives_batch = data[negatives_batch_idx]\n",
    "    positives_batch = positives[positives_batch_idx]\n",
    "    \n",
    "    if test_point is not None:\n",
    "        if positive:\n",
    "            x = np.vstack((negatives_batch,\n",
    "                           positives_batch,\n",
    "                           test_point.reshape((1, 2))))\n",
    "            y = np.vstack((\n",
    "                np.zeros((len(negatives_batch), 1)),\n",
    "                np.ones((len(positives_batch) + 1, 1)),\n",
    "            ))\n",
    "        else:\n",
    "            x = np.vstack((negatives_batch,\n",
    "                           test_point.reshape((1, 2)),\n",
    "                           positives_batch))\n",
    "            y = np.vstack((\n",
    "                np.zeros((len(negatives_batch) + 1, 1)),\n",
    "                np.ones((len(positives_batch), 1)),\n",
    "            ))\n",
    "    else:\n",
    "        x = np.vstack((negatives_batch,\n",
    "                       positives_batch))\n",
    "        y = np.vstack((\n",
    "            np.zeros((len(negatives_batch), 1)),\n",
    "            np.ones((len(positives_batch), 1)),\n",
    "        ))\n",
    "\n",
    "    loss_value, grads = grad(model, x, y)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(test_point, n_train_steps=10):\n",
    "    query_classifier = tf.keras.models.clone_model(classifier)\n",
    "    original_weights = classifier.get_weights()\n",
    "    query_classifier.set_weights(original_weights)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "    # Train with negative label\n",
    "\n",
    "    for i in range(n_train_steps):\n",
    "        do_training_iter(query_classifier,\n",
    "                         optimizer,\n",
    "                         test_point.reshape((1, 2)),\n",
    "                         positive=False)\n",
    "        \n",
    "#     do_training_iter(query_classifier,\n",
    "#                      optimizer,\n",
    "#                      test_point.reshape((1, 2)),\n",
    "#                      positive=False)\n",
    "#     for i in range(10):\n",
    "#         do_training_iter(query_classifier,\n",
    "#                          optimizer)\n",
    "        \n",
    "    p_minus = query_classifier.predict(test_point.reshape((1, 2)))\n",
    "    \n",
    "    # Plot\n",
    "    n_samples = 50\n",
    "    xs = np.linspace(-4, 4, n_samples)\n",
    "    ys = np.linspace(-4, 4, n_samples)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    xys = np.meshgrid(xs, ys)\n",
    "    grid_vals = np.array(xys).transpose(1, 2, 0).reshape((n_samples * n_samples, 2))\n",
    "\n",
    "    rewards = query_classifier.predict(grid_vals)\n",
    "\n",
    "    plt.contour(xys[0], xys[1], rewards.reshape(xys[0].shape), levels=1)\n",
    "#     plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.scatter(test_point[0], test_point[1], color='purple', marker='*', s=200)\n",
    "    plt.scatter(goal[0], goal[1], color='red', marker='*', s=200)\n",
    "    \n",
    "    # Train Again\n",
    "    query_classifier.set_weights(original_weights)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "    for i in range(n_train_steps):\n",
    "        do_training_iter(query_classifier,\n",
    "                         optimizer,\n",
    "                         test_point.reshape((1, 2)),\n",
    "                         positive=True)\n",
    "    \n",
    "#     do_training_iter(query_classifier,\n",
    "#                      optimizer,\n",
    "#                      test_point.reshape((1, 2)),\n",
    "#                      positive=True)\n",
    "#     for i in range(10):\n",
    "#         do_training_iter(query_classifier,\n",
    "#                          optimizer)\n",
    "        \n",
    "    p_plus = query_classifier.predict(test_point.reshape((1, 2)))\n",
    "    reward = p_plus / (p_plus + p_minus)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    rewards = query_classifier.predict(grid_vals)\n",
    "    plt.contour(xys[0], xys[1], rewards.reshape(xys[0].shape), levels=1)\n",
    "#     plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.scatter(test_point[0], test_point[1], color='purple', marker='*', s=200)\n",
    "    plt.scatter(goal[0], goal[1], color='red', marker='*', s=200)\n",
    "    \n",
    "    print(p_minus, p_plus, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reward(np.array([2.5, -2.5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:softlearning] *",
   "language": "python",
   "name": "conda-env-softlearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
